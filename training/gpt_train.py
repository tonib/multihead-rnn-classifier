from __future__ import annotations
from typing import Tuple

from training.base_train import BaseTrain
from dataset.transformer_dataset import TransformerDataset
from model.mingpt.model_adapted import GPT

import tensorflow as tf

# TODO: Loss reduction?

class GptTrain(BaseTrain):

    def __init__(self):
        super().__init__(TransformerDataset)

    def create_model(self) -> tf.keras.Model:
        return GPT.create_model(self.data_definition)

    @staticmethod
    def mask_outputs(y_true: tf.Tensor, y_pred: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:
        """ Masks output values to ignore positions with value TransformerDataset.OUTPUT_PADDING_VALUE) """
        # y_true are the labels to predict. They are padded on the right side: -1 == Pad. Shape (batch size, timesteps, )
        # y_pred are the logits generated by the model. Shape = (batch size, timesteps, number of labels)

        # Adapted from https://github.com/kamalkraj/minGPT-TF/blob/master/mingpt/trainer.py
        logits_shape = tf.shape(y_pred)
        num_labels = logits_shape[-1]
        label_mask = tf.math.logical_not(y_true < 0) # False -> Pad element here
        label_mask = tf.reshape(label_mask,(-1,)) # -> Flatten mask

        y_pred = tf.reshape(y_pred,(-1,num_labels)) # Remove batch size / timestep dimensions
        y_pred = tf.boolean_mask(y_pred,label_mask) # Keep predictions in non-padded positions (mask at axis zero)

        y_true = tf.reshape(y_true,(-1,)) # Flatten labels
        y_true = tf.boolean_mask(y_true, label_mask) # Keep non-padding labels

        return (y_true, y_pred)

    @staticmethod
    def gpt_loss(y_true: tf.Tensor, y_pred: tf.Tensor):
        """ Loss function to ignore masked elements (output positions with value TransformerDataset.OUTPUT_PADDING_VALUE) """
        # y_true are the labels to predict. They are padded on the right side: -1 == Pad. Shape (batch size, timesteps, )
        # y_pred are the logits generated by the model. Shape = (batch size, timesteps, number of labels)

        # Adapted from https://github.com/kamalkraj/minGPT-TF/blob/master/mingpt/trainer.py

        # Get original logits shape
        logits_shape = tf.shape(y_pred)

        # Remove masked values
        y_true, y_pred = GptTrain.mask_outputs(y_true, y_pred)

        # Compute loss for each single label
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)

        # TODO: Should we return a loss for each element, or a single scalar?
        # TODO: Originally, minGPT-TF computes a single scalar, but it seems a loss per batch element is required
        # TODO: See: https://github.com/tensorflow/tensorflow/issues/42446
        # Original y_pred shape cannot be keeped, as we have removed masked elements, so, I don't know
        batch_size = logits_shape[0]
        return tf.reduce_sum(cross_entropy) * (1.0 / tf.cast(batch_size, tf.float32) )

    @staticmethod
    def masked_accuracy(y_true: tf.Tensor, y_pred: tf.Tensor):
        """ Accuracy metric to ignore masked elements (output positions with value TransformerDataset.OUTPUT_PADDING_VALUE) """
        # y_true are the labels to predict. They are padded on the right side: -1 == Pad. Shape (batch size, timesteps, )
        # y_pred are the logits generated by the model. Shape = (batch size, timesteps, number of labels)

        # Remove masked values
        y_true, y_pred = GptTrain.mask_outputs(y_true, y_pred)

        return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)

    def create_losses(self):
        # Losses for each output (sum of all will be minimized)
        self.losses = {}
        for output_column_name in self.data_definition.output_columns:
            self.losses[output_column_name] = GptTrain.gpt_loss

    def create_metrics(self):
        self.metrics = [ GptTrain.masked_accuracy ]

    def print_summary(self):
        # Does not work for keras.Model subclassing. model.build() neither
        # model.summary()
        # The only way I have found to get a summary is to feed a real sample, this seems to compile the model. After that,
        # summary can be printed
        build_model_ds = self.train_dataset.dataset.take(1)
        for input, output in build_model_ds:
            self.model(input)
            self.model.summary()
